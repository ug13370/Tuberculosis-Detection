# -*- coding: utf-8 -*-
"""TB_detec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kT-wSsspHg06MxUX4oahpJxy7gGwwTup
"""

!nvidia-smi

from google.colab import drive
drive.mount('/content/drive')

!cp /content/drive/MyDrive/662_images/662_images.zip .
!unzip 662_images.zip

!cp /content/drive/MyDrive/7000_images/Dataset.zip .
!unzip Dataset.zip

# !cp /content/drive/MyDrive/images/seg.zip .

# !unzip seg.zip

import tensorflow

# Commented out IPython magic to ensure Python compatibility.
import numpy as np 

import pandas as pd
import os
import cv2
from cv2 import imread
# %matplotlib inline
import matplotlib.pyplot as plt
import random


categories = ['Normal','Tuberculosis']
datadir = "/content/Dataset/TB_Chest_Radiography_Database"

X_shape = 256
data = []
for category in categories:
  path = os.path.join(datadir,category)
  class_index = categories.index(category)
  for img in os.listdir(path):
    temp = cv2.imread(os.path.join(path,img))
    temp = cv2.resize(temp,(X_shape,X_shape))
    data.append([temp,class_index])

categories1 = ['NonTub','Tub']
datadir1 = "/content/662_images"

for category in categories1:
  path = os.path.join(datadir1,category)
  class_index = categories1.index(category)
  for img in os.listdir(path):
    temp = cv2.imread(os.path.join(path,img))
    temp = cv2.resize(temp,(X_shape,X_shape))
    data.append([temp,class_index])
len(data)

random.shuffle(data)

X = []
y = []
for feature,label in data:
  X.append(feature)
  y.append(label)

del data

X = np.array(X).reshape(len(X),X_shape,X_shape,3)
y = np.array(y)

plt.imshow(np.squeeze(X[1000]))
plt.show()

X = (X-127.0)/127.0

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25,random_state=42,shuffle = True)

del X
del y

from keras import layers
from keras.models import Sequential
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.applications import DenseNet201
# from keras.utils.np_utils import to_categorical
from keras.layers import  Conv2D,MaxPooling2D,Activation,Dropout,Flatten,Dense,BatchNormalization

def build_model(backbone, lr):
    model = Sequential()
    model.add(backbone)
    model.add(layers.GlobalAveragePooling2D())
    model.add(layers.Dropout(0.5))
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(1, activation='sigmoid'))
    
    model.compile(
        loss='binary_crossentropy',
        optimizer=Adam(lr=lr),
        metrics=['acc']
    )
    return model

densenet = DenseNet201(
    weights=None,
    include_top=False,
    input_shape=(X_shape,X_shape,3)
)

model = build_model(densenet ,lr = 2e-4)
model.summary()

from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau
weight_path="{}_weights_tb.best.hdf5".format('cxr_reg')

checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, 
                             save_best_only=True, mode='min', save_weights_only = True)

reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, 
                                   patience=3, 
                                   verbose=1, mode='min', epsilon=0.003, cooldown=2, min_lr=1e-6)
early = EarlyStopping(monitor="val_loss", 
                      mode="min", 
                      patience=15) 
callbacks_list = [checkpoint, early, reduceLROnPlat]

history = model.fit(X_train,y_train,batch_size=32,epochs=25,verbose=1,validation_split=0.25,callbacks=callbacks_list)

print("Evaluate on test data")
results = model.evaluate(X_test, y_test, batch_size=24)
print("test loss, test acc:", results)

len(y_test)

y_true=y_test
y_pred=model.predict(X_test)
for i in range(len(y_pred)):
  if y_pred[i]>0.5:
    y_pred[i]=1
  else:
    y_pred[i]=0
y_pred=y_pred.astype(int)

from sklearn.metrics import confusion_matrix
cm=confusion_matrix(y_true,y_pred)
cm

import seaborn as sns
import matplotlib.pyplot as plt

f, ax=plt.subplots(figsize=(5,5))
sns.heatmap(cm,annot=True,linewidths=0.5,linecolor="red",fmt=".0f",ax=ax)
plt.xlabel("y_pred")
plt.ylabel("y_true")
plt.show()

print(y_pred)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))
ax1.plot(history.history['loss'], '-', label = 'Loss')
ax1.plot(history.history['val_loss'], '-', label = 'Validation Loss')
ax1.set_ylabel('Loss')
ax1.set_xlabel('Epochs')
ax1.legend()

ax2.plot(100*np.array(history.history['acc']), '-', 
         label = 'Accuracy')
ax2.plot(100*np.array(history.history['val_acc']), '-',
         label = 'Validation Accuracy')
ax2.set_ylabel('Accuracy')
ax2.set_xlabel('Epochs')
ax2.legend()

from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score

from sklearn import metrics

print('Precision: %.7f' % precision_score(y_test, y_pred))

print('Recall: %.7f' % recall_score(y_test, y_pred))
print('Accuracy: %.7f' % accuracy_score(y_test, y_pred))
print('F1 Score: %.7f' % f1_score(y_test, y_pred))

print(metrics.classification_report(y_test,y_pred,digits=4))

temp = cv2.imread('/content/Tuberculosis-545.png')
temp = cv2.resize(temp,(X_shape,X_shape))

temp = np.array(temp).reshape(1,X_shape,X_shape,3)

plt.imshow(np.squeeze(temp[0]))
plt.show()

predict_ans = model.predict(temp)
# predict_ans = predict_ans.astype(np.int8)
if predict_ans > 0.5:
  print('1')
else:
  print('0')

predict_ans

